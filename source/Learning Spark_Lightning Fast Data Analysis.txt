spark快速大数据分析

安装在C:\spark目录
运行pyspark: C:\spark\spark-1.6.0-bin-hadoop2.6\bin>pyspark

第一个用例
>>> lines = sc.textFile("C:\\spark\\spark-1.6.0-bin-hadoop2.6\\README.md")		#如果文件不在C:\spark\spark-1.6.0-bin-hadoop2.6\bin下面，那就用绝对路径吧
>>> lines.count()
95
>>> lines.first()
u'# Apache Spark'
>>> pythonLines = lines.filter(lambda line: "Python" in line)			#筛选带"Python"的行
>>> pythonLines
PythonRDD[11] at RDD at PythonRDD.scala:43
>>> pythonLines.first()
u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that'


知识点；
进入pyspark环境，这个环境已经自动的帮你创建了一个“SparkContext对象”：sc
>>> sc
<pyspark.context.SparkContext object at 0x025BF2F0>

sc可以创建RDD（弹性分布式数据集），RDD的数据可以来源于文件，也可以来源于python对象（list，set等）

如果要编写独立运行的python脚本，那么sc的创建要自己做：
==
from pyspark import SparkConf, SparkContext

conf = SparkConf().setMaster("local").setAppName("My App")
sc = SparkContext(conf = conf)
#
#...
sc.stop()

#

==
运行的时候要使用下面的方式运行独立脚本
bin/spark-submit my_script.py


以上是快速入门，详细的入门在http://spark.apache.org/docs/latest/quick-start.html


RDD----1.创RDD建，2.转化已有RDD，3.调用RDD操作

sc.textFile等方式创建RDD后，每次操作会重新计算，这就会每次读取数据到内存。如果要在多个行动操作中重用同一个 RDD，可以使用RDD.persist() 让 Spark 把这个 RDD 缓存下来。
>>> pythonLines.persist()
PythonRDD[11] at RDD at PythonRDD.scala:43
>>> pythonLines.first()
u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that'
>>> pythonLines.count()
3

创建RDD
lines = sc.parallelize(["pandas", "i like pandas"])	或
lines = sc.textFile("/path/to/README.md")


转化操作


行动操作
>>> print "Input had " + str(lines.count()) + " concerning lines"
>>> for line in lines.take(10):
...     print line
...
>>>

RDD还有一个collect()函数，可以用来获取整个 RDD 中的数据----不过一定要保证collect()的RDD的规模小到可以放进执行动作的计算机


向Spark传递函数
Spark 的大部分转化操作和一部分行动操作，都需要依赖用户传递的函数来计算。

简单的函数可以用lambda
word = rdd.filter(lambda s: "error" in s)

map()	接收一个函数，把这个函数用于 RDD 中的每个元素，将函数的返回结果作为结果 RDD 中对应元素的值。
filter()	接收一个函数，并将 RDD 中满足该函数的元素放入新的RDD 中返回。

>>> nums = sc.parallelize([1, 2, 3, 4])
>>> squared = nums.map(lambda x: x * x).collect()
>>> for num in squared:
...     print "%i " % (num)
...
1
4
9
16
>>>


flatMap()		对每个输入元素生成多个输出元素.返回的不是一个元素，而是一个返回值序列的迭代器。
>>> lines = sc.parallelize(["hello world", "hi"])
>>> words = lines.flatMap(lambda line: line.split(" "))
>>> words.first()
'hello'
>>> words.count()
3
>>>

伪集合操作
RDD.distinct()							混洗（值唯一化）
RDD.union(otherRDD)					并集（如果原来的RDD有重复元素，union() 操作也会包含这些重复数据，即不会混洗）
RDD.intersection(otherRDD)	交集（运行时会去掉所有重复的元素，会混洗）
RDD.subtract(otherRDD)			返回只存在与RDD中而不存在于otherRDD的数据，会混洗


RDD.cartesian(otherRDD)			笛卡儿积（RDD和otherRDD的元素组合）


sample(withReplacement,fraction, [seed])			采样


行动操作
RDD.reduce(func)						它接收一个函数作为参数，这个函数要操作两个 RDD 的元素类型的数据并返回一个同样类型的新元素。
	sum = rdd.reduce(lambda x, y: x + y)

fold()
		num=sc.parallelize([1,2,3,4],1)			分1组 [1,2,3,4]
		num.fold(10,add)										result=10+(10+1+2+3+4)
		
		num=sc.parallelize([1,2,3,4],2)			分2组 [[1,2],[3,4]]
		num.fold(10,add)										result=10+(10+1+2)+(10+3+4)
		
		
		num=sc.parallelize([1,2,3,4],4)			分4组 [[1],[2],[3],[4]]
		num.fold(10,add)										result=10+(10+1)+(10+2)+(10+3)+(10+4)
		
		
		
		
aggregate()			聚合
	
nums=sc.parallelize([1,2,3,4,5,6,7,8,9],1)			分1组 [1,2,3,4]
sumCount = nums.aggregate((0, 0),(lambda acc, value: (acc[0] + value, acc[1] + 1)),(lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] +acc2[1])))

为了显示计算的过程，将lambda函数转化为一般函数：
nums=sc.parallelize([1,2,3,4,5,6,7,8,9],1)
def action1(x,y):
	print ("action1:")
	print ("x=", x, "y=",y)
	print ((x[0]+y, x[1]+1))
	return (x[0]+y, x[1]+1)

def action2(x,y):
	print ("action2:")
	print ("x=", x, "y=",y)
	print ((x[0]+y[0], x[1]+y[1]))
	return (x[0]+y[0], x[1]+y[1])
	
sumCount = nums.aggregate((0, 0),action1,action2)
输出：
action1:											由于只分了一组，所以下面的9个action1都是这个分组内的动作
('x=', (0, 0), 'y=', 1)
(1, 1)
action1:
('x=', (1, 1), 'y=', 2)
(3, 2)
action1:
('x=', (3, 2), 'y=', 3)
(6, 3)
action1:
('x=', (6, 3), 'y=', 4)
(10, 4)
action1:
('x=', (10, 4), 'y=', 5)
(15, 5)
action1:
('x=', (15, 5), 'y=', 6)
(21, 6)
action1:
('x=', (21, 6), 'y=', 7)
(28, 7)
action1:
('x=', (28, 7), 'y=', 8)
(36, 8)
action1:
('x=', (36, 8), 'y=', 9)
(45, 9)
action2:													分组内的操作结束以后，做组间的聚合，由于只有1个分组，那也没有什么组间了，之和init量聚合一次既可
('x=', (0, 0), 'y=', (45, 9))
(45, 9)


如果分为3组呢
nums=sc.parallelize([1,2,3,4,5,6,7,8,9],1)
函数action1和action2不变
sumCount = nums.aggregate((0, 0),action1,action2)
输出：
action1:										属于第3分组
('x=', (0, 0), 'y=', 7)
(7, 1)
action1:										属于第3分组
('x=', (7, 1), 'y=', 8)
(15, 2)
action1:										属于第3分组
('x=', (15, 2), 'y=', 9)
(24, 3)
action1:										属于第2分组
('x=', (0, 0), 'y=', 4)
(4, 1)
action1:										属于第2分组
('x=', (4, 1), 'y=', 5)
(9, 2)
action1:										属于第2分组
('x=', (9, 2), 'y=', 6)
(15, 3)
action1:										属于第1分组
('x=', (0, 0), 'y=', 1)
(1, 1)
action1:										属于第1分组
('x=', (1, 1), 'y=', 2)
(3, 2)
action1:										属于第1分组
('x=', (3, 2), 'y=', 3)
(6, 3)

action2:										组间聚合，现在3组的数据是(6, 3)，(15, 3)，(24, 3)
('x=', (0, 0), 'y=', (6, 3))		将第1组的聚合结果聚合
(6, 3)
action2:
('x=', (6, 3), 'y=', (15, 3))		将第2组的聚合结果聚合
(21, 6)
action2:
('x=', (21, 6), 'y=', (24, 3))	将第3组的聚合结果聚合
(45, 9)
>>>

求平均值
sumCount[0] / float(sumCount[1])


如果是分2组呢？
action1:											第2组5个数据的聚合
('x=', (0, 0), 'y=', 5)
(5, 1)
action1:
('x=', (5, 1), 'y=', 6)
(11, 2)
action1:
('x=', (11, 2), 'y=', 7)
(18, 3)
action1:
('x=', (18, 3), 'y=', 8)
(26, 4)
action1:
('x=', (26, 4), 'y=', 9)
(35, 5)


action1:											第1组4个数据的聚合
('x=', (0, 0), 'y=', 1)
(1, 1)
action1:
('x=', (1, 1), 'y=', 2)
(3, 2)
action1:
('x=', (3, 2), 'y=', 3)
(6, 3)
action1:
('x=', (6, 3), 'y=', 4)
(10, 4)


action2:												2组数据的聚合
('x=', (0, 0), 'y=', (10, 4))
(10, 4)
action2:
('x=', (10, 4), 'y=', (35, 5))
(45, 9)


RDD.collect()			将整个 RDD 的内容返回
>>> x=nums.collect()
>>> x
[1, 2, 3, 4, 5, 6, 7, 8, 9]


take(n) 					返回 RDD 中的 n 个元素，并且尝试只访问尽量少的分区，因此该操作会得到一个不均衡的集合。
top()							从 RDD 中获取前几个元素。
takeSample(withReplacement, num, seed)			函数可以让我们从数据中获取一个采样，并指定是否替换。
foreach()					对 RDD 中的所有元素应用一个行动操作，但是不把任何结果返回到驱动器程序中
count()
countByValue()
>>> nums.countByValue()
defaultdict(<type 'int'>, {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1})


RDD持久化(缓存)			让某些过程得到的RDD常驻各个计算节点，而不是让每个动作都虫头计算
RDD.persist()		前面提到过
RDD.unpersist()


以上已经是Spark的核心知识

第4章　键值对操作
pair RDD

创建Pair RDD：
>>> lines = sc.textFile("C:\\spark\\spark-1.6.0-bin-hadoop2.6\\README.md")
>>> lines.count()
95
>>> pairs = lines.map(lambda x: (x.split(" ")[0], x))		#将由文本行组成的 RDD 转换为以每行的第一个单词为键的 pair RDD
>>> pairs.count()
95
>>> pairs.first()
(u'#', u'# Apache Spark')

Pair RDD的转化操作
reduceByKey(func)			合并具有相同键的值
groupByKey()					对具有相同键的值进行分组
combineByKey(createCombiner,mergeValue,mergeCombiners,partitioner)使用不同的返回类型合并具有相同键的值
mapValues(func)				对pairRDD中的每个值应用一个函数而不改变键
flatMapValues(func)		对pair RDD 中的每个值应用一个返回迭代器的函数，然后对返回的每个元素都生成一个对应原键的键值对记录。通常用于符号化
keys()								返回一个仅包含键的RDD
values()							返回一个仅包含值的RDD
sortByKey()						返回一个根据键排序的RDD
针对两个pair RDD的转化操作
subtractByKey(other)	删掉 RDD 中键与 otherRDD 中的键相同的元素
join(other)						对两个 RDD进行内连接
rightOuterJoin				对两个 RDD进行连接操作，确保第一个 RDD 的键必须存在（右外连接）
leftOuterJoin					对两个 RDD进行连接操作，确保第二个 RDD 的键必须存在（左外连接）
cogroup								将两个 RDD中拥有相同键的数据分组到一起


reduceByKey举例
>>> x=[("panda", 0), ("pink", 3), ("pirate", 3), ("panda", 1), ("pink", 4)]
>>> data=sc.parallelize(x)
>>> data
ParallelCollectionRDD[97] at parallelize at PythonRDD.scala:423
>>> data.count()
5
>>> data.first()
('panda', 0)
>>>
>>> data.mapValues(lambda x: (x, 1)).collect()				每个数据转换为(x, 1)
[('panda', (0, 1)), ('pink', (3, 1)), ('pirate', (3, 1)), ('panda', (1, 1)), ('pink', (4, 1))]
>>>
>>> data.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])).collect()
[('pink', (7, 2)), ('panda', (1, 2)), ('pirate', (3, 1))]


统计字数
>>> lines = sc.textFile("C:\\spark\\spark-1.6.0-bin-hadoop2.6\\README.md")
>>> lines.count()
95
>>> words = lines.flatMap(lambda x: x.split(" "))
>>> words.take(20)
[u'#', u'Apache', u'Spark', u'', u'Spark', u'is', u'a', u'fast', u'and', u'general', u'cluster', u'computing', u'system', u'for'
, u'Big', u'Data.', u'It', u'provides', u'high-level', u'APIs']
>>> result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)
>>> result.collect()
[(u'', 67), (u'when', 1), (u'R,', 1), (u'including', 3), (u'computation', 1), (u'using:', 1), (u'guidance', 2), (u'Scala,', 1),
(u'environment', 1), (u'only', 1), (u'rich', 1), (u'Apache', 1), (u'sc.parallelize(range(1000)).count()', 1), (u'Building', 1),
(u'guide,', 1), (u'return', 2), ............(u'documentation', 3), (u'It', 2), (u'graphs', 1), (u'./dev/run-tests', 1), (u'first',
 1), (u'latest', 1)]
 
 
combineByKey()举例
>>> nums=sc.parallelize([1,2,3,4,5,6,7,8,9],1)

nums=[("coffee", 1), ("coffee", 2), ("panda", 3), ("coffee", 9)]
nums=sc.parallelize(nums, 2)

def action0(x):
	print ("action0:")
	print (x, (x,1))
	return (x, 1)

def action1(x,y):
	print ("action1:")
	print (x, y)
	print ((x[0]+y, x[1]+1))
	return (x[0]+y, x[1]+1)

def action2(x,y):
	print ("action2:")
	print (x, y)
	print ((x[0]+y[0], x[1]+y[1]))
	return (x[0]+y[0], x[1]+y[1])


>>> sumCount = nums.combineByKey(action0,action1,action2)		#前三个参数是三个函数，第一个函数是处理的数据的key第一次出线时要做的动作。第二个函数是处理的数据的key已经出现过的情况，第三个函数是在计算节点之间进行merge的动作。
>>>
>>> sumCount.count()
action0:
(3, (3, 1))
action0:
(9, (9, 1))
action0:
(1, (1, 1))
action1:
((1, 1), 2)
(3, 2)
action2:
((3, 2), (9, 1))
(12, 3)
2
>>>

>>> sumCount.collect()
action2:
((3, 2), (9, 1))
(12, 3)
[('coffee', (12, 3)), ('panda', (3, 1))]
>>>

>>>sumCount.map(lambda data: (data[0], data[1][0]/data[1][1])).collectAsMap()					lambda与书上的不一样，书上的提示错误，应该是我的这个才对
{'coffee': 4, 'panda': 3}						#平均值


4.3.2　数据分组
groupByKey() 会使用 RDD中的键来对数据进行分组。
>>> nums=sc.parallelize({(1, 2), (3, 4), (3,6)})
>>> nums.groupByKey().collect()
[(1, <pyspark.resultiterable.ResultIterable object at 0x0527F790>), (3, <pyspark.resultiterable.ResultIterable object at 0x0527F
6D0>)]
>>> x=nums.groupByKey().collect()
>>> x[1][1].data
[4, 6]
>>>



4.3.3　连接
将有键的数据与另一组有键的数据一起使用是对键值对数据执行的最有用的操作之一
连接方式多种多样：右外连接、左外连接、交叉连接以及内连接。

内连接---只保留共有key的部分
>>> rdd1=sc.parallelize({("coffee", 1), ("coffee", 2), ("panda", 3)})
>>> rdd2=sc.parallelize({("coffee", 9), ("panda", 3), ("pirate", 5)})
>>> rdd1.collect()
[('coffee', 2), ('coffee', 1), ('panda', 3)]
>>> rdd2.collect()
[('pirate', 5), ('coffee', 9), ('panda', 3)]
>>> rdd3=rdd1.join(rdd2)
>>> rdd3.collect()
[('coffee', (2, 9)), ('coffee', (1, 9)), ('panda', (3, 3))]
>>>

如果不想像内连接那样只保留共有key的项，就需要使用leftOuterJoin(other)和rightOuterJoin(other)
>>> rdd1=sc.parallelize({("coffee", 1), ("coffee", 2), ("panda", 3), ("tiger", 7)})
>>> rdd2=sc.parallelize({("coffee", 9), ("panda", 3), ("pirate", 5)})
>>> rdd1.collect()
[('coffee', 2), ('coffee', 1), ('tiger', 7), ('panda', 3)]
>>> rdd2.collect()
[('pirate', 5), ('coffee', 9), ('panda', 3)]
>>> rdd3=rdd1.leftOuterJoin(rdd2)
>>> rdd3.collect()
[('tiger', (7, None)), ('coffee', (2, 9)), ('coffee', (1, 9)), ('panda', (3, 3))]
>>> rdd4=rdd1.rightOuterJoin(rdd2)
>>> rdd4.collect()
[('coffee', (2, 9)), ('coffee', (1, 9)), ('panda', (3, 3)), ('pirate', (None, 5))]
>>>

4.3.4　数据排序
sortByKey()				可以升序降序，也可以指定比较函数

>>> rdd1.collect()
[('coffee', 2), ('coffee', 1), ('tiger', 7), ('panda', 3)]
>>> rdd3=rdd1.sortByKey(ascending=True, numPartitions=None, keyfunc = lambda x:str(x))
>>> rdd3.collect()
[('coffee', 2), ('coffee', 1), ('panda', 3), ('tiger', 7)]
>>>

Pair RDD的行动操作
countByKey()												对每个键对应的元素分别计数
collectAsMap()											将结果以映射表的形式返回，以便查询
lookup(key)													返回给定键对应的所有值


Map{(1, 2),(3, 4), (3,6)}


数据分区
Python-Spark中对数据的分区比Scala简单。只要指定要分多少个区既可。内部对自动的使用hash等方式分区。如果要改变hash函数，也可以指定
import urlparse
def hash_domain(url):
	return hash(urlparse.urlparse(url).netloc)
RDD.partitionBy(20,hash_domain)


第 5 章　数据读取与保存
当数据太大，保存在多台计算机上，就需要考虑数据读取与保存问题了

三种常见的数据源
1.文件格式与文件系统
2.Spark SQL中的结构化数据源
3.数据库与键值存储


读取文本文件
读单个文件：input = sc.textFile("C:\\spark\\spark-1.6.0-bin-hadoop2.6\\README.md")
读多个文件：input = sc.wholeTextFiles("D:\\README.md,D:\\README2.md")								得到的是pairRDD，键值是文件名
保存文本文件：result.saveAsTextFile(outputFile)

读取JSON
import json
data = input.map(lambda x: json.loads(x))
保存JSON
假设我们要选出喜爱熊猫的人
data.filter(lambda x: x["lovesPandas"]).map(lambda x: json.dumps(x)).saveAsTextFile(outputFile)

读取CSV
import csv
import StringIO
def loadRecord(line):
	"""解析一行CSV记录"""
	input = StringIO.StringIO(line)
	reader = csv.DictReader(input, fieldnames=["name", "favouriteAnimal"])
	return reader.next()
input = sc.textFile(inputFile).map(loadRecord)

def loadRecords(fileNameContents):
	"""读取给定文件中的所有记录"""
	input = StringIO.StringIO(fileNameContents[1])
	reader = csv.DictReader(input, fieldnames=["name", "favoriteAnimal"])
	return reader
fullFileData = sc.wholeTextFiles(inputFile).flatMap(loadRecords)


保存CSV
def writeRecords(records):
	"""写出一些CSV记录"""
	output = StringIO.StringIO()
	writer = csv.DictWriter(output, fieldnames=["name", "favoriteAnimal"])
	for record in records:
		writer.writerow(record)
	return [output.getvalue()]
pandaLovers.mapPartitions(writeRecords).saveAsTextFile(outputFile)



SequenceFile 是由没有相对关系结构的键值对文件组成的常用 Hadoop格式。SequenceFile 也是 Hadoop MapReduce作业中常用的输入输出格式
读取SequenceFile
data = sc.sequenceFile(inFile,"org.apache.hadoop.io.Text", "org.apache.hadoop.io.IntWritable")
保存SequenceFile
data = sc.parallelize(list([("Panda", 3), ("Kay", 6), ("Snail", 2)]))
data.saveAsSequenceFile("D:\\xxx")



对象文件
对象文件看起来就像是对 SequenceFile 的简单封装，它允许存储只包含值的 RDD。和 SequenceFile 不一样的是，对象文件是使用 Java 序列化写出的。
对象文件的数据对应java的类，如果类有调整，那原对象文件就不可使用了。
对象文件在 Python 中无法使用


Hadoop输入输出格式
Python不支持？


文件压缩
略

文件系统
本地/“常规”文件系统
Spark 支持从本地文件系统中读取文件，不过它要求文件在集群中所有节点的相同路径下都可以找到。
如果网络文件系统将某个网络上的文件映射到各个计算节点的同一个路径位置，那么Spark可以当作本地文件处理


Amazon S3
略

HDFS----Hadoop 分布式文件系统
在 Spark 中使用 HDFS 只需要将输入输出路径指定为hdfs://master:port/path


Spark SQL中的结构化数据
把一条 SQL 查询给 Spark SQL，让它对一个数据源执行查询（选出一些字段或者对字段使用一些函数），然后得到由 Row对象组成的 RDD，每个 Row 对象表示一条记录。
用Spark SQL操作Apache Hive
from pyspark.sql import HiveContext
hiveCtx = HiveContext(sc)			#HiveContext对象，也就是Spark SQL的入口
rows = hiveCtx.sql("SELECT name, age FROM users")
firstRow = rows.first()
print firstRow.name

用Spark SQL操作JSON

假设有文件d:\tweets.json，内容是
{"user": {"name": "Holden", "location": "San Francisco"}, "text": "Nice day out today"}
{"user": {"name": "Matei", "location": "Berkeley"}, "text": "Even nicer here :)"}

代码
from pyspark.sql import HiveContext
hiveCtx = HiveContext(sc)
tweets = hiveCtx.jsonFile("d:\\tweets.json")
tweets.registerTempTable("tweets")
results = hiveCtx.sql("SELECT user.name,text FROM tweets")



7.3　使用spark-submit部署应用
Spark 为各种集群管理器提供了统一的工具来提交作业，这个工具是 spark-submit。
提交 Python 应用的例子
========================================
bin/spark-submit my_script.py


如果在调用 spark-submit 时除了脚本或 JAR 包的名字之外没有别的
参数，那么这个 Spark 程序只会在本地执行。当我们希望将应用提交到
Spark 独立集群上的时候，可以将独立集群的地址和希望启动的每个执
行器进程的大小作为附加标记提供，如：
========================================
bin/spark-submit --master spark://host:7077 --executor-memory 10g my_script.py

--master
spark://host:port
mesos://host:port
local				运行本地模式，使用单核
local[N]		运行本地模式，使用 N 个核心
local[*]		运行本地模式，使用尽可能多的核心

7.5　Spark应用内与应用间调度
处理多个用户在同一个共享集群上各自提交作业的情况。


7.6　集群管理器

7.6.1　独立集群管理器
			如何构建Spark集群
7.6.2　Hadoop YARN
7.6.3　Apache Mesos
7.6.4　Amazon EC2


第 8 章　Spark 调优与调试


第 9 章　Spark SQL


9.2.1　初始化Spark SQL			这里都假设未引入hive依赖

from pyspark.sql import SQLContext, Row
sqlCtx = SQLContext(sc)				#在 Python 中创建 SQL 上下文环境

9.2.2　基本查询示例
input = sqlCtx.read.json("D:\\MyLearning\\Spark\\testweet.json")			#在spark1.3以后，input是pyspark.sql.dataframe.DataFrame类型的
					
# 注册输入的SchemaRDD
input.registerTempTable("tweets")			#把这些数据注册为一张临时表并赋予该表一个名字

input.count()

依据retweetCount（转发计数）选出推文
input.select("text","retweetCount").filter("retweetCount=0").show()
或者
input.select(input.text,input.retweetCount).filter(input.retweetCount==0).show()
或者
sqlCtx.sql("select text,retweetCount from tweets where retweetCount=0").show()

使用Row对象
topTweets = sqlCtx.sql("select text,retweetCount from tweets where retweetCount=0")			topTweets是DataFrame
topTweets.take(1)
	[Row(text=u'Adventures With Coffee, Code, and Writing.', retweetCount=0)]

topTweetText = topTweets.map(lambda row: row.text)												topTweetText是RDD						
topTweetText2 = topTweets.map(lambda row: (row.text, row.retweetCount))

9.2.4　缓存

9.3　读取和存储数据
	9.3.1　Apache Hive
	9.3.2　Parquet
	9.3.3　JSON
	9.3.4　基于RDD			除了读取数据，也可以基于RDD创建DataFrame
		happyPeopleRDD = sc.parallelize([Row(name="holden", favouriteBeverage="coffee")])
		happyPeopleSchemaRDD = sqlCtx.inferSchema(happyPeopleRDD)
		happyPeopleSchemaRDD.registerTempTable("happy_people")


9.4　JDBC/ODBC服务器
	Spark SQL 也提供 JDBC 连接支持，这对于让商业智能（BI）工具连接到 Spark 集群上以及在多用户间共享一个集群的场景都非常有用。
		我理解，JDBC(Java Database Connectivity)是系统对外可以提供的Java风格的数据库接口。Spark SQL对外
		提供了符合JDBC标准的接口，这让使用JDBC的程序可以无感知的使用Spart集群中的数据。
		
	Beeline是Hive新的命令行客户端工具。是基于SQLLine CLI的JDBC客户端。
	用Beeline访问Spark SQL提供的JDBC服务器
	
	
	用Beeline或其他访问JDBC的客户端访问Spark SQL JDBC服务器前，要先启动Spark SQL JDBC服务器
		./sbin/start-thriftserver.sh --master sparkMaster
	然后使用Beeline访问
		./bin/beeline -u jdbc:hive2://localhost:10000
	

9.5　用户自定义函数
用户自定义函数，也叫 UDF，可以让我们使用 Python/Java/Scala 注册自
定义函数，并在 SQL 中调用。这种方法很常用，通常用来给机构内的
SQL 用户们提供高级功能支持，这样这些用户就可以直接调用注册的函
数而无需自己去通过编程来实现了。

def myFoo(x):
	print ("myFoo")
	print (x)
	return len(x)

sqlCtx.registerFunction("strLenPython", lambda x: len(x))
sqlCtx.registerFunction("strLenPython", myFoo)
lengthSchemaRDD = sqlCtx.sql("SELECT strLenPython('textxxx') FROM tweets LIMIT 10")
lengthSchemaRDD.show()



第 10 章　Spark Streaming


Spark Streaming用于处理流式数据

内部的实现方式是将流数据切割成固定时间间隔的RDD

处理流数据，要使用StreamingContext()创建对象，这个对象接收各类来源的数据，生成DStream
DStream 是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为RDD 存在，而 DStream 是由这些 RDD 所组成的序列。


Spark Streaming会在底层创建出 SparkContext，进行每个RDD的数据处理，这些是程序员不直接可见的。在程序员的视角，需要
1.创建StreamingContext对象
2.该对象从指定的数据源创建DStream
3.指定DStream要做的各种传化操作和输出操作
4.StreamingContext对象调用start()启动处理
5.StreamingContext对象调用awaitTermination()等待任务完成（对有些流数据处理，会一直处理下去，不会停止）


10.3　转化操作
DStream 的转化操作可以分为无状态（stateless）和有状态（stateful）两种。
区别在于数据处理时是否依赖之前数据的计算结果

10.3.1　无状态转化操作
无状态转化操作就是把简单的 RDD 转化操作应用到每个批次上，也就是转化 DStream 中的每一个 RDD。
主要有
map()
flatMap()					对 DStream 中的每个元素应用给定函数，返回由各元素输出的迭代器组成的DStream。
filter()
repartition()			改变 DStream 的分区数。
reduceByKey()			将每个批次中键相同的记录归约。
groupByKey()			将每个批次中的记录根据键分组。

这些无状态转化操作是分别应用到DStream内部的每个RDD上的

无状态转化操作也能在多个 DStream 间整合数据，不过也是在各个时间区间内。DStream1和DStream2中有相同数据段的RDD可以跨DStream整合数据
cogroup()、join()、leftOuterJoin()

如果以上的操作无法满足要求，DStream 还提供了一个叫作transform() 的高级操作符，可以让你直接操作其内部的 RDD。这个
transform() 操作允许你对 DStream 提供任意一个 RDD 到 RDD 的函数。
这个函数会在数据流中的每个批次中被调用，生成一个新的流。transform() 的一个常见应用就是重用你为 RDD 写的批处理代码。


10.3.2　有状态转化操作

DStream 的有状态转化操作是跨时间区间跟踪数据的操作；也就是说，一些先前批次的数据也被用来在新的批次中计算结果。
主要的两种类型是滑动窗口和 updateStateByKey()，前者以一个时间阶段为滑动窗口进行操作，后者则用来跟踪每个键的状
态变化（例如构建一个代表用户会话的对象）。

基于窗口的转化操作
基于窗口的操作会在一个比 StreamingContext 的批次间隔更长的时间范
围内，通过整合多个批次的结果，计算出整个窗口的结果。

两个参数，分别为窗口时长以及滑动步长

比如某DStream中有10秒为单位的RDD数据序列
-  -  -  -  0  1  2  3  4  5  6  7  8
如果窗口为30秒，滑动步长是20秒
那么初始态是
-  -  -  -  0  1  2  3  4  5  6  7  8
   |窗口 |
窗口第1批数据----滑动20秒，取30秒，窗口中只有20秒的数据（0，1）
-  -  -  -  0  1  2  3  4  5  6  7  8
		     |窗口 |
窗口第2批数据----滑动20秒，取30秒，窗口中有30秒的数据（1，2，3）
-  -  -  -  0  1  2  3  4  5  6  7  8
		           |窗口 |
窗口第3批数据----滑动20秒，取30秒，窗口中有30秒的数据（3，4，5）
-  -  -  -  0  1  2  3  4  5  6  7  8
		                 |窗口 |

确定窗口时长以及滑动步长以后，就可以开始处理了。

对DStream可以用的最简单窗口操作是 window()，它返回一个新的DStream来表示所请求的窗口操作的结果数据。
换句话说，window()生成的 DStream 中的每个 RDD 会包含多个批次中的数据，可以对这些数据进行 count()、transform() 等操作

既然是窗口操作，那么每一次进行滑窗就会有老的数据离开，新的数据加入。那么对很多操作。没必要重新计算窗口内的全部数据。
只需要消除掉老数据，增加新数据既可。

reduceByWindow()
reduceByKeyAndWindow()
countByWindow()
countByValueAndWindow()
updateStateByKey()				跨批次维护状态（例如跟踪用户访问网站的会话）


reduceByWindow() 和 reduceByKeyAndWindow() 让我们可以对每个窗口更高效地进行归约操作。
它们接收一个归约函数，在整个窗口上执行，比如 +。除此以外，它们还有一种特殊形式，通过只考虑新进
入窗口的数据和离开窗口的数据，让 Spark 增量计算归约结果。这种特殊形式需要提供归约函数的一个逆函数，
比如 + 对应的逆函数为 -。对于较大的窗口，提供逆函数可以大大提高执行效率。
简单说就是这两个函数要提供如何处理新增数据的函数，也要提供如何处理旧数据的函数。

countByWindow() 和countByValueAndWindow() 作为对数据进行计数操作的简写。

updateStateByKey()跨批次维护状态。它的参数是一个update(events,oldState) 函数，接收与某键相关的事件以及该键之前对应的状态，返回这个键对应的新状态。


10.4　输出操作

print()			常用的一种调试性输出操作，它会在每个批次中抓取DStream 的前十个元素打印出来。
saveAsTextFiles("outputDir", "txt")			每个批次的结果被保存在给定目录的子目录中，且文件名中含有时间和后缀名。
saveAsHadoopFiles()
foreachRDD()		通用的输出操作，对DStream 中的 RDD 运行任意计算。对其中的RDD，可以继续使用eachPartition()遍历每一条数据，并完成具体的操作。如存入MySql


10.5　输入源


10.6　24/7不间断运行
要不间断运行 Spark Streaming 应用，需要一些特别的配置

10.6.1　检查点机制
检查点机制是我们在 Spark Streaming 中用来保障容错性的主要机制。它可以使 Spark Streaming 阶段性地把应用数据存储到诸如 HDFS 或
Amazon S3 这样的可靠存储系统中，以供恢复时使用。检查点机制对于任何生产环境中的流计算应用都至关重要。

你可以通过向 ssc.checkpoint() 方法传递一个路径参数（HDFS、S3 或者本地路径均可）来配置检查点机制，如
ssc.checkpoint("hdfs://...")

10.6.2　驱动器程序容错
10.6.3　工作节点容错
10.6.4　接收器容错
10.6.5　处理保证


10.7　Streaming用户界面


第 11 章基于 MLlib 的机器学习



	


