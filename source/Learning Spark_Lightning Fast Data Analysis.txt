spark快速大数据分析

安装在C:\spark目录
运行pyspark: C:\spark\spark-1.6.0-bin-hadoop2.6\bin>pyspark

第一个用例
>>> lines = sc.textFile("C:\\spark\\spark-1.6.0-bin-hadoop2.6\\README.md")		#如果文件不在C:\spark\spark-1.6.0-bin-hadoop2.6\bin下面，那就用绝对路径吧
>>> lines.count()
95
>>> lines.first()
u'# Apache Spark'
>>> pythonLines = lines.filter(lambda line: "Python" in line)			#筛选带"Python"的行
>>> pythonLines
PythonRDD[11] at RDD at PythonRDD.scala:43
>>> pythonLines.first()
u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that'


知识点；
进入pyspark环境，这个环境已经自动的帮你创建了一个“SparkContext对象”：sc
>>> sc
<pyspark.context.SparkContext object at 0x025BF2F0>

sc可以创建RDD（弹性分布式数据集），RDD的数据可以来源于文件，也可以来源于python对象（list，set等）

如果要编写独立运行的python脚本，那么sc的创建要自己做：
==
from pyspark import SparkConf, SparkContext

conf = SparkConf().setMaster("local").setAppName("My App")
sc = SparkContext(conf = conf)
#
#...
sc.stop()

#

==
运行的时候要使用下面的方式运行独立脚本
bin/spark-submit my_script.py


以上是快速入门，详细的入门在http://spark.apache.org/docs/latest/quick-start.html


RDD----1.创RDD建，2.转化已有RDD，3.调用RDD操作

sc.textFile等方式创建RDD后，每次操作会重新计算，这就会每次读取数据到内存。如果要在多个行动操作中重用同一个 RDD，可以使用RDD.persist() 让 Spark 把这个 RDD 缓存下来。
>>> pythonLines.persist()
PythonRDD[11] at RDD at PythonRDD.scala:43
>>> pythonLines.first()
u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that'
>>> pythonLines.count()
3

创建RDD
lines = sc.parallelize(["pandas", "i like pandas"])	或
lines = sc.textFile("/path/to/README.md")


转化操作


行动操作
>>> print "Input had " + str(lines.count()) + " concerning lines"
>>> for line in lines.take(10):
...     print line
...
>>>

RDD还有一个collect()函数，可以用来获取整个 RDD 中的数据----不过一定要保证collect()的RDD的规模小到可以放进执行动作的计算机


向Spark传递函数
Spark 的大部分转化操作和一部分行动操作，都需要依赖用户传递的函数来计算。

简单的函数可以用lambda
word = rdd.filter(lambda s: "error" in s)

map()	接收一个函数，把这个函数用于 RDD 中的每个元素，将函数的返回结果作为结果 RDD 中对应元素的值。
filter()	接收一个函数，并将 RDD 中满足该函数的元素放入新的RDD 中返回。

>>> nums = sc.parallelize([1, 2, 3, 4])
>>> squared = nums.map(lambda x: x * x).collect()
>>> for num in squared:
...     print "%i " % (num)
...
1
4
9
16
>>>


flatMap()		对每个输入元素生成多个输出元素.返回的不是一个元素，而是一个返回值序列的迭代器。
>>> lines = sc.parallelize(["hello world", "hi"])
>>> words = lines.flatMap(lambda line: line.split(" "))
>>> words.first()
'hello'
>>> words.count()
3
>>>

伪集合操作
RDD.distinct()							混洗（值唯一化）
RDD.union(otherRDD)					并集（如果原来的RDD有重复元素，union() 操作也会包含这些重复数据，即不会混洗）
RDD.intersection(otherRDD)	交集（运行时会去掉所有重复的元素，会混洗）
RDD.subtract(otherRDD)			返回只存在与RDD中而不存在于otherRDD的数据，会混洗


RDD.cartesian(otherRDD)			笛卡儿积（RDD和otherRDD的元素组合）


sample(withReplacement,fraction, [seed])			采样


行动操作
RDD.reduce(func)						它接收一个函数作为参数，这个函数要操作两个 RDD 的元素类型的数据并返回一个同样类型的新元素。
	sum = rdd.reduce(lambda x, y: x + y)

fold()
		num=sc.parallelize([1,2,3,4],1)			分1组 [1,2,3,4]
		num.fold(10,add)										result=10+(10+1+2+3+4)
		
		num=sc.parallelize([1,2,3,4],2)			分2组 [[1,2],[3,4]]
		num.fold(10,add)										result=10+(10+1+2)+(10+3+4)
		
		
		num=sc.parallelize([1,2,3,4],4)			分4组 [[1],[2],[3],[4]]
		num.fold(10,add)										result=10+(10+1)+(10+2)+(10+3)+(10+4)
		
		
		
		
aggregate()			聚合
	
nums=sc.parallelize([1,2,3,4,5,6,7,8,9],1)			分1组 [1,2,3,4]
sumCount = nums.aggregate((0, 0),(lambda acc, value: (acc[0] + value, acc[1] + 1)),(lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] +acc2[1])))

为了显示计算的过程，将lambda函数转化为一般函数：
nums=sc.parallelize([1,2,3,4,5,6,7,8,9],1)
def action1(x,y):
	print ("action1:")
	print ("x=", x, "y=",y)
	print ((x[0]+y, x[1]+1))
	return (x[0]+y, x[1]+1)

def action2(x,y):
	print ("action2:")
	print ("x=", x, "y=",y)
	print ((x[0]+y[0], x[1]+y[1]))
	return (x[0]+y[0], x[1]+y[1])
	
sumCount = nums.aggregate((0, 0),action1,action2)
输出：
action1:											由于只分了一组，所以下面的9个action1都是这个分组内的动作
('x=', (0, 0), 'y=', 1)
(1, 1)
action1:
('x=', (1, 1), 'y=', 2)
(3, 2)
action1:
('x=', (3, 2), 'y=', 3)
(6, 3)
action1:
('x=', (6, 3), 'y=', 4)
(10, 4)
action1:
('x=', (10, 4), 'y=', 5)
(15, 5)
action1:
('x=', (15, 5), 'y=', 6)
(21, 6)
action1:
('x=', (21, 6), 'y=', 7)
(28, 7)
action1:
('x=', (28, 7), 'y=', 8)
(36, 8)
action1:
('x=', (36, 8), 'y=', 9)
(45, 9)
action2:													分组内的操作结束以后，做组间的聚合，由于只有1个分组，那也没有什么组间了，之和init量聚合一次既可
('x=', (0, 0), 'y=', (45, 9))
(45, 9)


如果分为3组呢
nums=sc.parallelize([1,2,3,4,5,6,7,8,9],1)
函数action1和action2不变
sumCount = nums.aggregate((0, 0),action1,action2)
输出：
action1:										属于第3分组
('x=', (0, 0), 'y=', 7)
(7, 1)
action1:										属于第3分组
('x=', (7, 1), 'y=', 8)
(15, 2)
action1:										属于第3分组
('x=', (15, 2), 'y=', 9)
(24, 3)
action1:										属于第2分组
('x=', (0, 0), 'y=', 4)
(4, 1)
action1:										属于第2分组
('x=', (4, 1), 'y=', 5)
(9, 2)
action1:										属于第2分组
('x=', (9, 2), 'y=', 6)
(15, 3)
action1:										属于第1分组
('x=', (0, 0), 'y=', 1)
(1, 1)
action1:										属于第1分组
('x=', (1, 1), 'y=', 2)
(3, 2)
action1:										属于第1分组
('x=', (3, 2), 'y=', 3)
(6, 3)

action2:										组间聚合，现在3组的数据是(6, 3)，(15, 3)，(24, 3)
('x=', (0, 0), 'y=', (6, 3))		将第1组的聚合结果聚合
(6, 3)
action2:
('x=', (6, 3), 'y=', (15, 3))		将第2组的聚合结果聚合
(21, 6)
action2:
('x=', (21, 6), 'y=', (24, 3))	将第3组的聚合结果聚合
(45, 9)
>>>

求平均值
sumCount[0] / float(sumCount[1])


如果是分2组呢？
action1:											第2组5个数据的聚合
('x=', (0, 0), 'y=', 5)
(5, 1)
action1:
('x=', (5, 1), 'y=', 6)
(11, 2)
action1:
('x=', (11, 2), 'y=', 7)
(18, 3)
action1:
('x=', (18, 3), 'y=', 8)
(26, 4)
action1:
('x=', (26, 4), 'y=', 9)
(35, 5)


action1:											第1组4个数据的聚合
('x=', (0, 0), 'y=', 1)
(1, 1)
action1:
('x=', (1, 1), 'y=', 2)
(3, 2)
action1:
('x=', (3, 2), 'y=', 3)
(6, 3)
action1:
('x=', (6, 3), 'y=', 4)
(10, 4)


action2:												2组数据的聚合
('x=', (0, 0), 'y=', (10, 4))
(10, 4)
action2:
('x=', (10, 4), 'y=', (35, 5))
(45, 9)


RDD.collect()			将整个 RDD 的内容返回
>>> x=nums.collect()
>>> x
[1, 2, 3, 4, 5, 6, 7, 8, 9]


take(n) 					返回 RDD 中的 n 个元素，并且尝试只访问尽量少的分区，因此该操作会得到一个不均衡的集合。
top()							从 RDD 中获取前几个元素。
takeSample(withReplacement, num, seed)			函数可以让我们从数据中获取一个采样，并指定是否替换。
foreach()					对 RDD 中的所有元素应用一个行动操作，但是不把任何结果返回到驱动器程序中
count()
countByValue()
>>> nums.countByValue()
defaultdict(<type 'int'>, {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1})


RDD持久化(缓存)			让某些过程得到的RDD常驻各个计算节点，而不是让每个动作都虫头计算
RDD.persist()		前面提到过
RDD.unpersist()


以上已经是Spark的核心知识

第4章　键值对操作
pair RDD

创建Pair RDD：
>>> lines = sc.textFile("C:\\spark\\spark-1.6.0-bin-hadoop2.6\\README.md")
>>> lines.count()
95
>>> pairs = lines.map(lambda x: (x.split(" ")[0], x))		#将由文本行组成的 RDD 转换为以每行的第一个单词为键的 pair RDD
>>> pairs.count()
95
>>> pairs.first()
(u'#', u'# Apache Spark')

Pair RDD的转化操作
reduceByKey(func)			合并具有相同键的值
groupByKey()					对具有相同键的值进行分组
combineByKey(createCombiner,mergeValue,mergeCombiners,partitioner)使用不同的返回类型合并具有相同键的值
mapValues(func)				对pairRDD中的每个值应用一个函数而不改变键
flatMapValues(func)		对pair RDD 中的每个值应用一个返回迭代器的函数，然后对返回的每个元素都生成一个对应原键的键值对记录。通常用于符号化
keys()								返回一个仅包含键的RDD
values()							返回一个仅包含值的RDD
sortByKey()						返回一个根据键排序的RDD
针对两个pair RDD的转化操作
subtractByKey(other)	删掉 RDD 中键与 otherRDD 中的键相同的元素
join(other)						对两个 RDD进行内连接
rightOuterJoin				对两个 RDD进行连接操作，确保第一个 RDD 的键必须存在（右外连接）
leftOuterJoin					对两个 RDD进行连接操作，确保第二个 RDD 的键必须存在（左外连接）
cogroup								将两个 RDD中拥有相同键的数据分组到一起


reduceByKey举例
>>> x=[("panda", 0), ("pink", 3), ("pirate", 3), ("panda", 1), ("pink", 4)]
>>> data=sc.parallelize(x)
>>> data
ParallelCollectionRDD[97] at parallelize at PythonRDD.scala:423
>>> data.count()
5
>>> data.first()
('panda', 0)
>>>
>>> data.mapValues(lambda x: (x, 1)).collect()				每个数据转换为(x, 1)
[('panda', (0, 1)), ('pink', (3, 1)), ('pirate', (3, 1)), ('panda', (1, 1)), ('pink', (4, 1))]
>>>
>>> data.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])).collect()
[('pink', (7, 2)), ('panda', (1, 2)), ('pirate', (3, 1))]


统计字数
>>> lines = sc.textFile("C:\\spark\\spark-1.6.0-bin-hadoop2.6\\README.md")
>>> lines.count()
95
>>> words = lines.flatMap(lambda x: x.split(" "))
>>> words.take(20)
[u'#', u'Apache', u'Spark', u'', u'Spark', u'is', u'a', u'fast', u'and', u'general', u'cluster', u'computing', u'system', u'for'
, u'Big', u'Data.', u'It', u'provides', u'high-level', u'APIs']
>>> result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)
>>> result.collect()
[(u'', 67), (u'when', 1), (u'R,', 1), (u'including', 3), (u'computation', 1), (u'using:', 1), (u'guidance', 2), (u'Scala,', 1),
(u'environment', 1), (u'only', 1), (u'rich', 1), (u'Apache', 1), (u'sc.parallelize(range(1000)).count()', 1), (u'Building', 1),
(u'guide,', 1), (u'return', 2), ............(u'documentation', 3), (u'It', 2), (u'graphs', 1), (u'./dev/run-tests', 1), (u'first',
 1), (u'latest', 1)]
 
 
10.5　输入源


10.6　24/7不间断运行
要不间断运行 Spark Streaming 应用，需要一些特别的配置

10.6.1　检查点机制
检查点机制是我们在 Spark Streaming 中用来保障容错性的主要机制。它可以使 Spark Streaming 阶段性地把应用数据存储到诸如 HDFS 或
Amazon S3 这样的可靠存储系统中，以供恢复时使用。检查点机制对于任何生产环境中的流计算应用都至关重要。

你可以通过向 ssc.checkpoint() 方法传递一个路径参数（HDFS、S3 或者本地路径均可）来配置检查点机制，如
ssc.checkpoint("hdfs://...")

10.6.2　驱动器程序容错
10.6.3　工作节点容错
10.6.4　接收器容错
10.6.5　处理保证


10.7　Streaming用户界面


第 11 章基于 MLlib 的机器学习



	


